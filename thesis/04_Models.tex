\chapter{Performances investigation}\label{sys-analysis}
After the simulator has been completed and tested, the phase of performances
investigation can start. The system has many free parameters that can be set to
get a realistic behaviour of the simulator, but also to investigate its
behaviour in a different condition. \\
The following sections will show the performance difference focusing on a single
parameter, showing how they will affect the total walltime. For each experiment
are done 5 different runs with a different value for that field, keeping also the
same request pattern. \\
The following is the standard configuration used across all the experiments. For
every different one a single parameter, or related ones, has been modified. \\
\small
\begin{tabular}{r | l | p{0.5\textwidth}}
    \textbf{Field} & \textbf{Value} & \textbf{Description}\\ \hline
    TOKEN\_COUNT & 24 & Client parallel communications\\
    GEOMETRY\_BASE & 3 & Data packets per parity group \\
    GEOMETRY\_PLUS & 3 & Parity packet per parity group\\
    SERVER\_COUNT & 20 & Server inside the system \\
    HDD\_DATA\_COUNT & 20 & Server Disks per server \\
    HDD\_DATA\_READ\_MBps & 2048 & Server Disk read performance in MBps\\
    HDD\_DATA\_WRITE\_MBps & 1024 & Server Disk write performance in MBps\\
    HDD\_DATA\_LATENCY\_us & 100 & Server Disk latency\\
    HUB\_BW\_Gbps & 80 & Network maximum bandwidth\\
    NETWORK\_LATENCY\_nS & 6667 & Network latency\\
    NETWORK\_BUFFER\_SIZE\_KB & 1024 & packet size used for communication\\
    BUCKET\_SIZE & 8192 & grouping size used for file allocation\\
    READ\_PATTERN & random & read pattern, see section \ref{read-pattern} \\
    READ\_BLOCK\_SIZE & 4 & linear read block size \\
\end{tabular}
\normalsize


\section{Geometry}
In this experiment has been investigated how the geometry affects performances.
Geometry is expressed as a pair of number $a+b$ where \textit{a} is is the data
packet per parity group and \textit{b} is the parity packets per parity group.
This is an option inside the system, meaning that parity packets can be used or
not based on user needs. The more parity is used the more data loss can be
recovered at the same time at the expense of performances. On the other side can
be turned off to communicate data as fast as possible aware of the fact that the
running experiment can not be recovered in case of hardware failure.\\
For this experiment has been used the standard configuration with the following
settings for base geometry \\
\begin{tabular}{r |r | l}
    \textbf{Run} & \textbf{Parameter} & \textbf{Value} \\\hline
    1 & GEOMETRY\_BASE & 2 \\
    2 & GEOMETRY\_BASE & 4 \\
    3 & GEOMETRY\_BASE & 6 \\
    4 & GEOMETRY\_BASE & 8 \\
    5 & GEOMETRY\_BASE & 10 \\
    5 & GEOMETRY\_BASE & 15 \\
    5 & GEOMETRY\_BASE & 20 \\
    5 & GEOMETRY\_BASE & 30 \\
    5 & GEOMETRY\_BASE & 40 \\
    5 & GEOMETRY\_BASE & 50 \\
\end{tabular}

\begin{myimage}{exp-geometry}{Performances using different geometries}
    Growing the base geometry size reduces the amount of overhead introduced in
    a system at the expense of recovery time, which keeps increasing
\end{myimage}

It's clear from figure 14 that while the time required to generate parity barely
decreases after a base geometry of 10, the recovery time still increases since
to recover every packet other $geometry_{base} + geometry_{plus}$ need to be
contacted, involving network communication and, in this case, congestion.
To better inspect the situation is better to not look at the single values but a
value using a formula involving bot the measures. Initially has been used a
simple average between the two measures, that highlited that the best size of
the geometry was the smallest, since the recovery time increases faster than the
parity generation time decreases. \\
But in reality happens that parity generation happens much often than recovery,
which hopefully should not happen at all. So the simple average has been
switched to a weighted average and different weights has been assigned. \\
In the first weighted average parity generation is 99/100 and the better
solution shifted from $x=2$ to $x=4$. \\
Using a different weight the solution shifted again to $x=15$, so in the case
where a parity generation happens 99.9\% of the time, is ideal a geometry of
15+1.\\
Keeping increasing the weight, representing a more realistic behaviour of the
simulator, suggest to increase as much as possible the size of the packet since
is very difficult that the restoration could happen, and in that case, is ideal
to wait for it to finish slowly but leaving the entire system with a very low
parity overhead. \\

\begin{tabular}{r | l | l | l | l }
    Geometry & Parity Gen.(ns) & Restore (ns) & Avg 99\% (ns) & Avg 99.9\% (ns) \\\hline
    2 & 101083 & 1103562 & 111107 & 102085 \\
    4 & 84512 & 1904740 & \textbf{102714}& 86332 \\
    6 & 78900 & 2723940 & 105350 & 81545 \\
    8 & 76173 & 3572632 & 111137 & 79669 \\
    10 & 74507 & 4476210 & 118524 & 78908 \\
    15 & 72508 & 6415274 & 135935 & \textbf{78850} \\
    20 & 71469 & 8532990 & 156084 & 79930 \\
    30 & 70440 & 11373276 & 183468 & 81742 \\
    40 & 69993 & 16756538 & 236858 & 86679 \\
    50 & 69738 & 21087047 & 279911 & 90755 \\
\end{tabular} \\

\section{Disk Bandwidth}\label{diskbw}
The technology involved in the servers is not the most powerful so the servers
can be improved. The standard settings in the simulator are close to the
performances of an SSD, reading speed at 2GB/s and writing speed at 1GB/s. The
usage of NVMe memories or PCIe ones can lead to better disk speeds. \\
This experiment run the tests with the following settings \\
\begin{tabular}{r |r | l}
    \textbf{Run} & \textbf{Parameter} & \textbf{Value} \\\hline
    1 & DISK\_WRITE & 512 \\
    2 & DISK\_WRITE & 1024 \\
    3 & DISK\_WRITE & 2048 \\
    4 & DISK\_WRITE & 3072 \\
    5 & DISK\_WRITE & 4096 \\
\end{tabular}

In this case the read bandwidth is supposed to be twice the write bandwidth. \\
To better investigate the influence of the disk bandwidth on the final time, a
setting with a more narrow bandwidth has been tested. If the results are
acceptably bad, IME could be designed to have cheaper drives and invest more on
more meaningful features.

\begin{myimage}{exp-diskbw}{Disk Bandwidth investigation}
Disk bandwidth R/W wall times using a different disk quantity per server
\end{myimage} \\
Figure 15 shows that the single disk performances really depends on the
configuration installed in a single server. In the extreme case where a single
disk is install per server, single disk performance really matters, being able
to reduce by a factor of 5 the total walltime for the write operation, as the
\emph{Write time 1 Disk} shows. \\
Nonetheless a real world server uses 20 disks per server and in this case with
such a parallel environment the single disks performance seems to not matter at
all. What happens in this case is that \cmloid s are stored in different disks,
so for such a small transaction is more important the disk latency than the disk
bandwidth, and that factor in this experiment has not been modified since this
should involve a different technology instead of a different SSD drive.

\section{Server count}
In this experiment the aim is to show which benefits can be obtained increasing
the number of servers. The aim of DDN is to be able to make use of thousands of
servers, being able to satisfy a lot of requests at the same time. \\
The setup used in the experiments are the following: \\
\begin{tabular}{r | r | l}
    \textbf{Run} & \textbf{Parameter} & \textbf{Value} \\\hline
    1 & SERVER\_COUNT & 4 \\
    2 & SERVER\_COUNT & 16 \\
    3 & SERVER\_COUNT & 64 \\
    4 & SERVER\_COUNT & 256 \\
    5 & SERVER\_COUNT & 3000 \\
\end{tabular} \\

A large amount of servers are useful to satisfy many more requests at the same
time, but they are not useful in case the clients involved in the simulation are
too few, even if they have a large amount of data to communicate. The firsts
runs failed to show any improvement because of too few clients involved in the
simulation. More clients has been instantiated communicating less data at the
same time. \\
The following are the results of the simulation with a different request pattern

Multiple tests has been conducted using different configurations. The 2
experiments shown in figure 16 are completed using a request of total size 256GB
and 8GB.\\
The test in this experiment are more close to a benchmark than a real case
simulation: since both read and write operation involved the whole dataset, to
better compare the recover operation also the latter has been performed over the
whole dataset. This is not possible in the real world because the system
could not survive to a complete disk failure at the same time, but for
simulation purposes is fair to compare different operation on the same
dataset.\\

\begin{myimage}{exp-servercount}{Server count experiment}
    The number of servers seems to not affect much the performance improvement.
    At the current time they only allow a bigger total disk capacity
\end{myimage} \\

As figure 16 shows increasing the number of server does not increase drastically
the overall performances. The experiment about the disk bandwidth in section
\ref{diskbw} showed that the time required for the I/O operations takes now a
small percentage of the total simulation wall time. The real bottleneck in this
situation is the network bandwidth: more requests take more time because of data
transfer from client to server and vice versa. To figure out the load of data to
be written from each single disk can be computed arithmetically.
Let 
$$ d = initial\ data $$
$$ D = total\ data $$
$$ g_b = geometry\ base $$
$$ g_+ = geometry\ plus $$
$$ s = total\ SSDs $$
$$ w = SSD's\ write\ speed $$

From simple arithmetic the total data \emph{D} and total time \emph{T} required
for write operations are
$$ D = \frac{d (g_b + g_+)}{g_b}$$
$$ T = \frac{d(g_b + g_+)}{g_bsw} $$ 

From this formula can be computed the following data:

\begin{tabular}{r | r | r| r| l}
    Init data(GB) & Geometry & Total SSDs & Write Speed(GB/s)& Total Time(s)\\\hline
    8 & 3+1 & 8Vj0 & 0.15 & 0.89 \\
    8 & 3+1 & 80 & 1 & 0.13 \\
    8 & 3+1 & 320 & 0.15 & 0.22 \\
    8 & 3+1 & 320 & 1 & 0.03 \\
    8 & 9+1 & 80 & 0.15 & 0.74 \\
    8 & 9+1 & 80 & 1 & 0.11 \\
    8 & 9+1 & 320 & 0.15 & 0.19 \\
    8 & 9+1 & 320 & 1 & 0.03 \\
    256 & 3+1 & 80 & 0.15 & 28.44 \\
    256 & 3+1 & 80 & 1 & 4.27 \\
    256 & 3+1 & 320 & 0.15 & 7.11 \\
    256 & 3+1 & 320 & 1 & 1.07 \\
    256 & 9+1 & 80 & 0.15 & 23.70 \\
    256 & 9+1 & 80 & 1 & 3.56 \\
    256 & 9+1 & 320 & 0.15 & 5.93 \\
    256 & 9+1 & 320 & 1 & 0.89 \\
\end{tabular} \\

Despite in the real simulation network traffic and some more mechanics makes the
data more variable, we can detect a baseline here. The parameter has been chosen
as follows:
\begin{itemize}
    \item Small and big transaction to show how much the transaction size matters
    \item Different geometry to show the influence of geometry
    \item Different server count to use a more realistic server configuration
    \item A write speed comparable to an HDD to show the importance of SSDs in
        this technology
\end{itemize}

The most realistic configuration is represented by the last line showing that
with a transaction relatively big, still the IO operations takes few time
compared to the total simulation wall time as show in figure 16. \\
To really make use of a big amount of servers, network should scale as well as
servers quantity, at least as total transaction time will be comparable to the
IO times.

